---
email:
  username: ${GMAIL_USERNAME}
  password: ${GMAIL_APP_PASSWORD}
  folder: "Google Scholar Alerts"

slack:
  api_token: ${SLACK_API_TOKEN}
  default_channel: "scholar-scout-default"
  channel_topics:
    scholar-scout-llm:
      - "LLM Inference"
    scholar-scout-serverless:
      - "Serverless Computing"


perplexity:
  api_key: ${PPLX_API_KEY}
  model: "sonar-pro"
research_topics:
  - name: "LLM Inference"
    keywords: ["llm", "inference", "serving", "latency", "throughput"]
    slack_users:
      - "<@U04MG63BX1R>" # "@Ruiqi Lai"
      - "<@U07J4DZRPB6>" # "@Hongrui Liu"
      - "<@U0618BHHTRR>" # "@Yulin"
      - "<@U058LPMQB6J>" # "@Colin Hong"
    slack_channel: "#systems-for-ai"
    description: >-
      Research related to LLM inference serving systems, architectures, and 
      applications, including latency optimization, throughput improvement, 
      and deployment strategies"

  - name: "Serverless Computing"
    keywords: ["serverless", "faas", "function-as-a-service", "cloud"]
    slack_users:
      - "<@U04HALEGCD7>" # "@Leonid Kondrashov"
      - "<@U04HVNW03QQ>" # "@Park JooYoung"
    slack_channel: "#serverless-systems"
    description: >-
      Research on serverless computing platforms, Function-as-a-Service (FaaS), 
      and cloud-native architectures

  - name: "Sustainable Computing"
    keywords: ["green computing", "energy efficiency", "carbon footprint"]
    slack_users:
      - "@jim"
    description: >-
      Research on reducing the environmental impact of computing systems, 
      including energy efficiency and carbon footprint reduction